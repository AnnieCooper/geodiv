---
title: "Assessing landscape heterogeneity in Oregon State"
author: "Annie C. Smith"
date: "September 5, 2019"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{oregon-heterogeneity}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Getting Started

First, import *geodiv* and any other necessary packages.

```{r setup, include = FALSE}

library(geodiv)
library(raster)
library(mapdata)
library(maptools)
library(rgeos)
library(ggplot2)

```

Now, open and plot a raster of the Enhanced Vegetation Index (EVI) for Oregon State. 

First, we want to open the data.
```{r data1}

# import EVI raster into R
data(oregonEVI)

```

Next, we want to take a look at what dataset includes. As it includes a scalar, we apply that value.

```{r data2}

# look up scalar and apply to data
?oregonEVI
oregonEVI <- oregonEVI * 0.0001

```

Now, we can make a map, masking any values outside of the Oregon state bounds.

```{r data3}

# mask any values that are outside of the state bounds
oregon <- map(database = 'state', regions = 'oregon', fill = TRUE, plot = FALSE)
oregonPoly <- map2SpatialPolygons(oregon, IDs = oregon$names, proj4string = CRS(proj4string(oregonEVI)))
oregonEVI <- mask(x = oregonEVI, mask = oregonPoly)

# plot maximum growing season EVI for Oregon
plot(oregonEVI, xlab = 'Longitude', ylab = 'Latitude', main = 'Maximum Growing Season EVI')

```

# Metric Calculations

##Removing the Best Fit Plane

For surface metrics, we don't really care about the actual values so much as their contrast. Because of this, we now want to remove any trend that there may be over the entire raster. This is made easy by a function included in *geodiv*, 'remove_plane.'

```{r remove trend}

# remove overall trend in values over raster (this removes the best-fit polynomial plane)
evi <- remove_plane(oregonEVI)

```

Note that this returned the order of polynomial that best fits any trend in the data. This is process is automated and considers polynomials orders of 1 -- 3. If you want a custom order of polynomial, you can use the 'fitplane' function to calculate the best fit plane of that order polynomial.

For now, let's keep the results of the 'remove_plane' function and take a look at the results.

```{r plot new evi}

# plot again to see what the new raster looks like
plot(evi, xlab = 'Longitude', ylab = 'Latitude', main = 'EVI without Trend')

```

## Sampling Locations

Now that we have a clean raster, we need to sample locations at which to calculate metrics. We will select uniformly-distributed points across the state to get a good representation of all ecosystems.

First, we create a grid with a cell size of 0.5 degrees over the raster area. We then convert the created grid to spatial points so that they may be plotted.

``` {r sample1, include = FALSE}

# create a grid of points across the state with 1/2 degree spacing
pt_grid <- makegrid(oregonPoly, cellsize = 0.5)
pt_grid <- SpatialPoints(pt_grid, proj4string = CRS(proj4string(evi)))

```

Now that we have points, we expect that some will fall outside of the state bounds. This is because the 'makegrid' function uses the extent of the input polygon, not the exact bounds. We clip (intersect) the points to the state bounds and convert those points into a dataframe. Finally, let's take a look at the results.

```{r sample2}

# cut to state outline
pt_grid <- gIntersection(pt_grid, oregonPoly)

# convert to sample data frame
samp_pts <- as.data.frame(coordinates(pt_grid))

# let's take a look in table form
head(samp_pts)

```

Let's also plot the resulting points to make sure they look correct.

```{r sample3}

# where are the points located on the map?
plot(evi, xlab = 'Longitude', ylab = 'Latitude', main = 'Sample Locations')
points(x = samp_pts$x, y = samp_pts$y, pch = 19)

```

Finally, determine the resulting sample size.

```{r sample4}

# how many points?
nrow(samp_pts)

```

This results in 108 sample points. At each of these points, we will calculate all surface metrics. First, we'll do this for a single point.

## Calculating metrics at sample locations

We run all the functions using a wrapper function that will print the metric being calculated and the result.

``` {r single example}

# create function to run all functions and output results
run_all <- function(r){

  # run all functions
  cat('Sa: ', '\n')
  sa <- sa(r)
  print(sa)
  cat('\n')

  cat('Sq: ', '\n')
  sq <- sq(r)
  print(sq)
  cat('\n')

  cat('S10z: ', '\n')
  s10z <-s10z(r)
  print(s10z)
  cat('\n')

  cat('Sdq: ', '\n')
  sdq <- sdq(r)
  print(sdq)
  cat('\n')

  cat('Sdq6: ', '\n')
  sdq6 <- sdq6(r)
  print(sdq6)
  cat('\n')

  cat('Sdr: ', '\n')
  sdr <- sdr(r)
  print(sdr)
  cat('\n')

  cat('Sbi: ', '\n')
  sbi <- sbi(r)
  print(sbi)
  cat('\n')

  cat('Sci: ', '\n')
  sci <- sci(r)
  print(sci)
  cat('\n')

  cat('Ssk (Adjacent): ', '\n')
  ssk_adj <- ssk(r, adj = TRUE)
  print(ssk_adj)
  cat('\n')

  cat('Sku (excess): ', '\n')
  sku_excess <- sku(r, excess = TRUE)
  print(sku_excess)
  cat('\n')

  cat('Sds: ', '\n')
  sds <- sds(r)
  print(sds)
  cat('\n')

  cat('Sfd: ', '\n')
  sfd <- sfd(as.matrix(r))
  print(sfd)
  cat('\n')

  cat('Srw function - no plotting: ', '\n')
  srwvals <- srw(r, plot = FALSE) # this takes a while
  print(srwvals)
  cat('\n')

  cat('Srw: ', '\n')
  srw <- srwvals[[1]]
  print(srw)
  cat('\n')

  cat('Srwi: ', '\n')
  srwi <- srwvals[[2]]
  print(srwi)
  cat('\n')

  cat('Shw: ', '\n')
  shw <- srwvals[[3]]
  print(shw)
  cat('\n')

  cat('Std function - no plotting: ', '\n')
  stdvals <- std(r, plot = FALSE)
  print(stdvals)
  cat('\n')

  cat('Std: ', '\n')
  std <- stdvals[[1]]
  print(std)
  cat('\n')

  cat('Stdi: ', '\n')
  stdi <- stdvals[[2]]
  print(stdi)
  cat('\n')

  cat('Svi: ', '\n')
  svi <- svi(r)
  print(svi)
  cat('\n')

  cat('Str function: ', '\n')
  strvals <- str(r, threshold = c(0.2, 1 / exp(1)))
  print(strvals)
  cat('\n')

  cat('Str (20%): ', '\n')
  str20 <- strvals[[1]]
  print(str20)
  cat('\n')

  cat('Str (37%): ', '\n')
  str37 <- strvals[[2]]
  print(str37)
  cat('\n')

  cat('Ssc: ', '\n')
  ssc <- ssc(r)
  print(ssc)
  cat('\n')

  cat('Sv: ', '\n')
  sv <- sv(r)
  print(sv)
  cat('\n')

  cat('Sp: ', '\n')
  sp <- sph(r)
  print(sp)
  cat('\n')

  cat('Sk: ', '\n')
  sk <- sk(r)
  print(sk)
  cat('\n')

  cat('Smean: ', '\n')
  smean <- smean(r)
  print(smean)
  cat('\n')

  cat('Spk: ', '\n')
  spk <- spk(r)
  print(spk)
  cat('\n')

  cat('Svk: ', '\n')
  svk <- svk(r)
  print(svk)
  cat('\n')

  cat('Scl function - no plotting: ', '\n')
  sclvals <- scl(r, threshold = c(0.2, 1 / exp(1)), plot = FALSE)
  print(sclvals)
  cat('\n')

  cat('Scl (20%): ', '\n')
  scl20 <- sclvals[[1]] # never gets to 0.2
  print(scl20)
  cat('\n')

  cat('Scl (37%): ', '\n')
  scl37 <- sclvals[[2]]
  print(scl37)
  cat('\n')

  cat('Sdc 0-5%: ', '\n')
  sdc0_5 <- sdc(r, 0, 0.05)
  print(sdc0_5)
  cat('\n')

  cat('Sdc 50-55%: ', '\n')
  sdc50_55 <- sdc(r, 0.50, 0.55)
  print(sdc50_55)
  cat('\n')

  cat('Sdc 80-85%: ', '\n')
  sdc80_85 <- sdc(r, 0.80, 0.85)
  print(sdc80_85)
  cat('\n')

  return(cat('Finished with raster.', '\n'))
}

```

Once we have the function written, we apply it to a single point.

Because the metrics are calculated on an area around each point, we have to clip the raster first. We'll use a box 0.25 degrees from the point in all directions.

We want to see where the sample location is, so we plot that against the entire state.

```{r single point1}
# select a point
samp_pt <- samp_pts[10,]

# calculate smaller extent surrounding point (0.25 degrees on from center to all sides)
bound_box <- extent(samp_pt$x - 0.25, samp_pt$x + 0.25, samp_pt$y - 0.25, samp_pt$y + 0.25)
bound_box <- as(bound_box, 'SpatialPolygons')

# crop the raster
samp_evi <- crop(evi, bound_box)

# plot the box
plot(evi, xlab = 'Longitude', ylab = 'Latitude', main = 'Sample Location for Test')
plot(bound_box, add = TRUE)

```

We then want to know what the variables represent, so we look at the sample location in more detail.

```{r single point2}

# look at the cropped area
plot(samp_evi, xlab = 'Longitude', ylab = 'Latitude', main = 'Sample Location for Test')

```

Finally, we run the function to calculate each metric and look at the results.

```{r single point3}

# run all functions on this test area
run_all(samp_evi)

```

This can take a while, primarily due to the Srw (radial wavelength), Scl (correlation lengths), and Str (texture aspect ratio) functions.

These functions take longer because they are extracting raster values over several lines. We can see this functionality using the plotting option in the 'srw' and 'std' functions. Str is the ratio between the fastest and slowest decay lengths to specified autocorrelation values along the lines visualized with the 'std' function.

```{r plot extraction functions}

srwvals <- srw(samp_evi, plot = TRUE) 
stdvals <- std(samp_evi, plot = TRUE)

```

While these functions take awhile, we want to know how all functions are useful for distinguishing ecosystems, so we'll leave them in for now.

Now we want to run the functions over all points and output the results to a dataframe. This will require re-writing the *run_all* function slightly and looping over all sampled points.

``` {r all points}

# create modified run_all function that outputs results to dataframe
# run_all <- function(r){
# 
#   # run all functions
#   sa <- sa(r)
#   sq <- sq(r)
#   s10z <-s10z(r)
#   sdq <- sdq(r)
#   sdq6 <- sdq6(r)
#   sdr <- sdr(r)
#   sbi <- sbi(r)
#   sci <- sci(r)
#   ssk_adj <- ssk(r, adj = TRUE)
#   sku_excess <- sku(r, excess = TRUE)
#   sds <- sds(r)
#   sfd <- sfd(as.matrix(r))
#   srwvals <- srw(r, plot = FALSE)
#   srw <- srwvals[[1]]
#   srwi <- srwvals[[2]]
#   shw <- srwvals[[3]]
#   stdvals <- std(r, plot = FALSE)
#   std <- stdvals[[1]]
#   stdi <- stdvals[[2]]
#   svi <- svi(r)
#   strvals <- str(r, threshold = c(0.2, 1 / exp(1)))
#   str20 <- strvals[[1]]
#   str37 <- strvals[[2]]
#   ssc <- ssc(r)
#   sv <- sv(r)
#   sp <- sph(r)
#   sk <- sk(r)
#   smean <- smean(r)
#   spk <- spk(r)
#   svk <- svk(r)
#   sclvals <- scl(r, threshold = c(0.2, 1 / exp(1)), plot = FALSE)
#   scl20 <- sclvals[[1]]
#   scl37 <- sclvals[[2]]
#   sdc0_5 <- sdc(r, 0, 0.05)
#   sdc50_55 <- sdc(r, 0.50, 0.55)
#   sdc80_85 <- sdc(r, 0.80, 0.85)
# 
#   result_df <- data.frame(x = mean(coordinates(r)[, 1]), y = mean(coordinates(r)[, 2]),
#                           metric = c('sa', 'sq', 's10z', 'sdq', 'sdq6', 'sdr', 'sbi', 'sci',
#                                      'ssk', 'sku', 'sds', 'sfd', 'srw', 'srwi', 'shw', 
#                                      'std', 'stdi', 'svi', 'str20', 'str37', 'ssc', 'sv',
#                                      'sp', 'sk', 'smean', 'spk', 'svk', 'scl20', 'scl37',
#                                      'sdc0_5', 'sdc50_55', 'sdc80_85'), 
#                           value = c(sa, sq, s10z, sdq, sdq6, sdr, sbi, sci, ssk_adj,
#                                     sku_excess, sds, sfd, srw, srwi, shw, std, stdi, svi,
#                                     str20, str37, ssc, sv, sp, sk, smean, spk, svk, scl20,
#                                     scl37, sdc0_5, sdc50_55, sdc80_85))
#   return(result_df)
# }
# 
# # loop over all points, cropping raster and running all functions for each
# for (i in seq(1, nrow(samp_pts))){
#   # write out what we're doing
#   cat('Calculating metrics for point: ', i, '\n')
#   
#   # select point
#   samp_pt <- samp_pts[i,]
# 
#   # calculate smaller extent surrounding point (0.25 degrees on from center to all sides)
#   bound_box <- extent(samp_pt$x - 0.25, samp_pt$x + 0.25, samp_pt$y - 0.25, samp_pt$y + 0.25)
#   bound_box <- as(bound_box, 'SpatialPolygons')
#   
#   # crop the raster
#   samp_evi <- crop(evi, bound_box)
#   
#   # if irregular, remove rows with NA vals
#   # get rows/cols with nas
#   na_col <- which(colSums(is.na(samp_evi)) > 0)
#   na_row <- which(rowSums(is.na(samp_evi)) > 0)
#   if (length(na_col) + length(na_row) != 0) {
#       # list only good rows/cols
#     good_cols <- seq(1, ncol(samp_evi))[-na_col]
#     good_rows <- seq(1, nrow(samp_evi))[-na_row]
#     # crop raster
#     new_ext <- extent(samp_evi, min(good_rows), max(good_rows), min(good_cols), max(good_cols))
#     samp_evi <- crop(samp_evi, new_ext)
#   }
# 
#   # calculate metrics
#   out <- run_all(samp_evi)
#   out$point <- i
#   
#   # add dataframes (or create if first point)
#   if (i == 1){
#     result <- out
#   } else {
#     result <- rbind(result, out)
#   }
#   
#   # remove data
#   rm(samp_pt, bound_box, samp_evi, out)
# }
# 
# # write out result
# write.csv(result, '~/Documents/surface_metrics/oregon_metric_results.csv', row.names = F)

# read in the future, because the above takes a while
result <- read.csv('~/Documents/surface_metrics/oregon_metric_results.csv')

```

## Visualization

We now have metrics for all of the sampling locations. We want to see if the metrics result in any natural grouping of the sites. We'll start investigating this by exploring the raw results. Let's look at distributions of a few metrics first. 

```{r map metrics}

data <- result

# distributions of a few variables
hist(data$value[data$metric == 'sa'], breaks = 30)
hist(data$value[data$metric == 'sbi'], breaks = 30)
hist(data$value[data$metric == 'sfd'], breaks = 30)
hist(data$value[data$metric == 'std'], breaks = 30)

```

Histograms demonstrate some variation in the values at the sampling locations, but this doesn't tell us very much about how those values relate to various ecosystem properties. To look at those relationships, let's map the metrics across the state.

```{r map metrics}

# map the texture direction index
or_ggpoly <- map_data('state', 'oregon')
ggplot() +
  geom_polygon(data = or_ggpoly, aes(x = long, y = lat, group = region), fill = 'white', colour = 'black') +
  geom_point(data = data[data$metric == 'std',], aes(x = x, y = y, size = value))

```

There aren't really any clear patterns in the map of texture direction, but combining metrics may improve our ability to distinguish ecosystems. 

We now want to investigate the observed groupings using a statistical approach that simplifies the many metrics into fewer groups, principal components analysis. The composition of the various components provides information on which metrics contain similar information. By observing how the first two or three principal components relate to one another, we can potentially split sites into ecosystems with various characteristics.

First, we have to clean up the data by reshaping it and removing any missing values.

```{r pca}

library(tidyverse)

# transform data into correct format (columns for each metric)
data <- spread(data = data, key = metric, value = value)

# order neatly
data <- data[order(data$point),]

# we should now have 108 neatly ordered rows
nrow(data)
head(data)

# check for missing values
sapply(data[, c(4:30, 32:35)], function(x) sum(is.na(x)))

# str20 has one missing value, which makes sense because it means that autocorrelation
# never decreases to 20% in that area
# for now, we will remove str20 because it correlates extremely highly with str37
plot(data$str20, data$str37)

# same with scl20, which is Inf for the same location (found by looking at the df)

```

We located a couple of missing values in the texture aspect ratio (Str) and correlation length functions (Scl). 

This makes a lot of sense, because the correlation length function determines the distance it takes to get the autocorrelation within the raster down to a specified value. Here, we see that the distance for an autocorrelation value of 37% is successfully estimated, but the distance for an autocorrelation value of 20% is not. This simply indicates that the raster values are highly autocorrelated in this area and do not ever decrease to 20%. We can see this high autocorrelation by looking at the autocorrelation function image.

```{r autocorrelation}

# show autocorrelation image for sample point with missing str20 and scl20 values

```

Now that we have investigated the data and decided which variables we can and want to use, we calculate the principal components using the function 'prcomp.'

```{r pca1}

# calculate principal components
data_prc <- prcomp(data[, c(4:7, 9:30, 32:35)], center = TRUE, scale = TRUE)
summary(data_prc)

# take a look at the components
screeplot(data_prc, type = "l", npcs = 15, main = "Screeplot of the first 10 PCs")
abline(h = 1, col = "red", lty = 5)
legend("topright", legend = c("Eigenvalue = 1"),
       col = c("red"), lty = 5, cex = 0.6)

cumpro <- cumsum(data_prc$sdev ^ 2 / sum(data_prc$sdev ^ 2))
plot(cumpro[0:15], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(v = 6, col = "blue", lty = 5)
abline(h = 0.7833145, col = "blue", lty = 5)
legend("topleft", legend = c("Cut-off @ PC6"),
       col = c("blue"), lty = 5, cex = 0.6)

# the first 2 components explain 54% of the variance, while the first three explain 62%

# let's plot the first 2
plot(data_prc$x[, 1], data_prc$x[, 2], xlab = "PC1 (32.1%)", ylab = "PC2 (22.1%)", main = "PC1 / PC2 - plot")

plot(data_prc$x[, 1], data_prc$x[, 3], xlab = "PC1 (32.1%)", ylab = "PC3 (8.0%)", main = "PC1 / PC3 - plot")

plot(data_prc$x[, 2], data_prc$x[, 3], xlab = "PC2 (22.1%)", ylab = "PC3 (8.0%)", main = "PC2 / PC3 - plot")

# how about mapping - are there any patterns?
data$prc1 <- data_prc$x[, 1]
data$prc2 <- data_prc$x[, 2]
data$prc3 <- data_prc$x[, 3]

# for plotting, convert EVI values to dataframe
evi_pixdf <- as(oregonEVI, "SpatialPixelsDataFrame")
evi_df <- as.data.frame(evi_pixdf)
colnames(evi_df) <- c("value", "x", "y")

# principal component 1
ggplot() +
  geom_tile(data = evi_df, aes(x = x, y = y, fill = value), alpha = 0.8) + 
  geom_point(data = data, aes(x = x, y = y, size = prc1))

# principal component 2
ggplot() +
  geom_tile(data = evi_df, aes(x = x, y = y, fill = value), alpha = 0.8) + 
  geom_point(data = data, aes(x = x, y = y, size = prc2))

# principal component 3
ggplot() +
  geom_tile(data = evi_df, aes(x = x, y = y, fill = value), alpha = 0.8) + 
  geom_point(data = data, aes(x = x, y = y, size = prc3))

```

While the individual principal components don't show any clustering, we can see what they represent when we map the results. The first principal component appears to be linked with areas of high contrast in EVI values (e.g., borders of forest and urban areas), while the second clearly represents the drier eastern side of the state versus the wetter western side of the state. It isn't clear what the third component represents; however, the primary variables included in that component are the ten-point height of EVI and the summit curvature around the highest peaks. Both variables should be linked to areas of how 'peaked' the EVI surface appears.

We investigate these patterns further by applying thresholds to the map to find low or high areas.

```{r apply clustering}

# what do various cutoffs for principal component 1 show?
ggplot() +
  geom_tile(data = evi_df, aes(x = x, y = y, fill = value), alpha = 0.8) + 
  geom_point(data = data[data$prc1 <= 50,], aes(x = x, y = y, size = prc1))

# what do various cutoffs for principal component 2 show?
ggplot() +
  geom_tile(data = evi_df, aes(x = x, y = y, fill = value), alpha = 0.8) + 
  geom_point(data = data[data$prc2 >= 0.5,], aes(x = x, y = y, size = prc2))

# what do various cutoffs for principal component 3 show?


```

The second principal component is clearly associated with lower and flatter EVI values, as all positive values of the component are in the unforested or relatively unforested areas of Oregon.

discuss results. explain mapping of clusters.

```{r map clusters}
```

final discussion of results and implications
